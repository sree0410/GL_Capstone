# -*- coding: utf-8 -*-
"""Capstone_Modelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kcpB1mpnWqa32KL6iqd_WRhAWImnOlYF

<h2 style="text-align:center;font-size:200%;;">Industrial Accident Classification by Accident Level </h2>
<h3  style="text-align:center;font-weight:bold">Steps :</h3>

<span class="label label-success">Feature Engineering</span> <br>
<span class="label label-success">Sampling</span> <br>
<span class="label label-success">Deep Learning Algorithms</span> <br>

# 1. Import libraries
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk import tokenize,stem
import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
import seaborn as sns

import os
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', -1)

from nltk.tokenize import word_tokenize
from tqdm import tqdm
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
from sklearn.feature_extraction.text import TfidfVectorizer

# pre-processing methods
from sklearn.model_selection import train_test_split


# sampling methods
from sklearn.utils import resample
from imblearn.over_sampling import SMOTE


# the classification models 
from sklearn.dummy import DummyClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import RidgeClassifier
from sklearn.linear_model import Lasso
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

# ensemble models
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier

from lightgbm import LGBMClassifier
from xgboost import XGBClassifier

# methods and classes for evaluation
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold

from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score, precision_score, roc_auc_score

# cross-validation methods
from sklearn.model_selection import KFold
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# feature selection methods
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.decomposition import TruncatedSVD
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from sklearn.feature_selection import mutual_info_classif
from sklearn.feature_selection import RFE
from sklearn.feature_selection import RFECV

# pre-processing methods
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PowerTransformer
from sklearn.preprocessing import LabelEncoder

# Deep learning libraries
from keras.utils import np_utils
#from keras.utils import plot_model
from keras.layers import Input
from keras.layers.merge import Concatenate
from keras.optimizers import SGD
from tensorflow.keras.models import Sequential
from tensorflow.keras import optimizers
from keras.models import Model
from tensorflow.keras.layers import Flatten, Activation, Dense, LSTM, BatchNormalization, Embedding, Dropout, Flatten, Bidirectional, GlobalMaxPool1D
from keras.models import model_from_json
from keras.regularizers import l1, l2, l1_l2
from keras.constraints import maxnorm, min_max_norm
from keras.constraints import unit_norm
from keras.callbacks import ReduceLROnPlateau
from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint
from keras.models import model_from_json

from keras.models import load_model
from keras.wrappers.scikit_learn import KerasClassifier

# Keras pre-processing
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

"""# 2. Load the dataset"""

from google.colab import drive 
drive.mount("/content/gdrive",force_remount=True)

import os; 
os.chdir('/content/gdrive/MyDrive/')

df = pd.read_csv("./AIML_CapstoneProject_NLP_Chat-bot_Cleaned.csv")
df.head()

"""## Rename Features"""

df.drop("Unnamed: 0", axis=1, inplace=True)
df.rename(columns={'Data':'Date', 'Countries':'Country', 'Local':'City', 'Genre':'Gender', 'Employee or Third Party':'Employee type'}, inplace=True)
df.head()

df.info()

"""## Feature Engineering"""

#original_df=df.copy(deep=True)

"""# Word2Vec Embedding"""

from gensim.models import Word2Vec
# define training data
sentences = df['Description_processed']

# train model
model = Word2Vec(sentences, min_count=1)

# summarize the loaded model
print(model)

# save model
model.save('wordvec_model.bin')

# load model
new_model = Word2Vec.load('wordvec_model.bin')
print(new_model)

"""# Glove Embedding"""

embeddings_index = {}
EMBEDDING_FILE = '/content/gdrive/MyDrive/glove.6B.200d.txt'
f = open(EMBEDDING_FILE)
for line in tqdm(f):
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('\nFound %s word vectors.' % len(embeddings_index))

def sent2vec(s):
    words = str(s).lower()
    words = word_tokenize(words)
    words = [w for w in words if not w in stop_words]
    words = [w for w in words if w.isalpha()]
    M = []
    for w in words:
        try:
            M.append(embeddings_index[w])
        except:
            continue
    M = np.array(M)
    v = M.sum(axis=0)
    if type(v) != np.ndarray:
        return np.zeros(300)
    return v / np.sqrt((v ** 2).sum())

# create sentence GLOVE embeddings vectors using the above function for training and validation set
ind_glove_df = [sent2vec(x) for x in tqdm(df['Description_processed'])]

"""# TFIDF Features"""

ind_tfidf_df = pd.DataFrame()
for i in [1,2,3]:
    vec_tfidf = TfidfVectorizer(max_features=10, norm='l2', stop_words='english', lowercase=True, use_idf=True, ngram_range=(i,i))
    X = vec_tfidf.fit_transform(df['Description_processed']).toarray()
    tfs = pd.DataFrame(X, columns=["TFIDF_" + n for n in vec_tfidf.get_feature_names()])
    ind_tfidf_df = pd.concat([ind_tfidf_df.reset_index(drop=True), tfs.reset_index(drop=True)], axis=1)

ind_tfidf_df.head(3)

df.info()

"""# One hot encoding for all features"""

# Dummy variables encoding
Country_dummies = pd.get_dummies(df['Country'], columns=["Country"], prefix='Country',drop_first=True)
Local_dummies = pd.get_dummies(df['City'], columns=["City"], prefix='City',drop_first=True)
Gender_dummies = pd.get_dummies(df['Gender'], columns=["Gender"], prefix='Gender', drop_first=True)
IS_dummies = pd.get_dummies(df['Industry Sector'], columns=['Industry Sector'], prefix='Industry', drop_first=True)
EmpType_dummies = pd.get_dummies(df['Employee type'], columns=['Employee type'], prefix='EmpType', drop_first=True)
CR_dummies = pd.get_dummies(df['Critical Risk'], columns=['Critical Risk'], prefix='CR', drop_first=True)

# Merge the above dataframe with the original dataframe ind_feat_df
df_one_hot_encoded = df.join(Country_dummies.reset_index(drop=True))
df_one_hot_encoded.drop('Country',inplace=True,axis=1)
df_one_hot_encoded = df_one_hot_encoded.join(Local_dummies.reset_index(drop=True))
df_one_hot_encoded.drop('City',inplace=True,axis=1)

df_one_hot_encoded = df_one_hot_encoded.join(Gender_dummies.reset_index(drop=True))
df_one_hot_encoded.drop('Gender',inplace=True,axis=1)

df_one_hot_encoded = df_one_hot_encoded.join(IS_dummies.reset_index(drop=True))
df_one_hot_encoded.drop('Industry Sector',inplace=True,axis=1)

df_one_hot_encoded = df_one_hot_encoded.join(EmpType_dummies.reset_index(drop=True))
df_one_hot_encoded.drop('Employee type',inplace=True,axis=1)

df_one_hot_encoded = df_one_hot_encoded.join(CR_dummies.reset_index(drop=True))
df_one_hot_encoded.drop('Critical Risk',inplace=True,axis=1)


##.join(Local_dummies.reset_index(drop=True)).join(Gender_dummies.reset_index(drop=True)).join(IS_dummies.reset_index(drop=True)).join(EmpType_dummies.reset_index(drop=True)).join(CR_dummies.reset_index(drop=True))

df_one_hot_encoded.info()

"""# Drop a few columns which are not relevant"""

df_one_hot_encoded.drop(labels=['Year','Month','Day','Weekday','WeekofYear','Potential Accident Level'],inplace=True,axis=1)

df_one_hot_encoded.drop(labels=['Date','Season'],inplace=True,axis=1)

df_one_hot_encoded.drop(labels=['Description'],inplace=True,axis=1)

df_one_hot_encoded.info()

"""# Join original data & glove vector"""

# Consider only top 30 GLOVE features
df_glove = df_one_hot_encoded.join(pd.DataFrame(ind_glove_df).iloc[:,0:30].reset_index(drop=True))
df_glove.head(1)



"""# Join original data & tf-idf vector"""

df_tfidf = df_one_hot_encoded.join(ind_tfidf_df.reset_index(drop=True))

df_tfidf.head(1)

df_tfidf.drop(labels=['Description_processed'],inplace=True,axis=1)
df_tfidf.head()

"""#UPSAMPLING before splitting data"""

# Display accident level counts
df_tfidf['Accident Level'].value_counts()

# Get the majority and minority class
acclevel_0_majority = df_tfidf[df_tfidf['Accident Level'] == 0]
acclevel_1_minority = df_tfidf[df_tfidf['Accident Level'] == 1]
acclevel_2_minority = df_tfidf[df_tfidf['Accident Level'] == 2]
acclevel_3_minority = df_tfidf[df_tfidf['Accident Level'] == 3]
acclevel_4_minority = df_tfidf[df_tfidf['Accident Level'] == 4]

# Upsample Level1 minority class
acclevel_1_minority_upsampled = resample(acclevel_1_minority,
                                replace = True, # sample with replacement
                                n_samples = len(acclevel_0_majority), # to match majority class
                                random_state = 1)

# Upsample Level2 minority class
acclevel_2_minority_upsampled = resample(acclevel_2_minority,
                                replace = True, # sample with replacement
                                n_samples = len(acclevel_0_majority), # to match majority class
                                random_state = 1)

# Upsample Level3 minority class
acclevel_3_minority_upsampled = resample(acclevel_3_minority,
                                replace = True, # sample with replacement
                                n_samples = len(acclevel_0_majority), # to match majority class
                                random_state = 1)

# Upsample Level4 minority class
acclevel_4_minority_upsampled = resample(acclevel_4_minority,
                                replace = True, # sample with replacement
                                n_samples = len(acclevel_0_majority), # to match majority class
                                random_state = 1)

# Combine majority class with upsampled minority classes
df_upsampld = pd.concat([acclevel_0_majority, acclevel_1_minority_upsampled, acclevel_2_minority_upsampled, acclevel_3_minority_upsampled, 
                          acclevel_4_minority_upsampled])

df_upsampld['Accident Level'].value_counts()

df_upsampld.dropna()

"""# Prepare dependent and independent variables"""

X = df_upsampld.drop(labels=['Accident Level'], axis = 1) # Considering all Predictors
y = df_upsampld['Accident Level']

#For neural network
X_train, X_test, Y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 1, stratify = y)
X_train, X_cv, y_train, y_cv = train_test_split(X_train, Y_train, test_size = 0.20, random_state = 1, stratify = Y_train)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 1, stratify = y)

y_test.value_counts()

y_train.value_counts()

df_upsampld.shape

"""#Importing the standard scaler library to scale the attributes"""

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()

X_train_std=scaler.fit_transform(X_train)
X_test_std=scaler.transform(X_test)

"""# 1. Machine Learning Algorithms


"""

from sklearn.metrics import classification_report,confusion_matrix, f1_score, accuracy_score, recall_score, precision_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB,MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier

"""#### Adaboost"""

abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())
parameters = {'base_estimator__max_depth':[i for i in range(2,11,2)],
              'base_estimator__min_samples_leaf':[5,10],
              'n_estimators':[10,50,250,1000],
              'learning_rate':[0.01,0.1]}

clf_ada_boost = GridSearchCV(abc, parameters,verbose=3,scoring='accuracy',n_jobs=-1)
clf_ada_boost.fit(X_train_std, y_train)

print("Best Params",clf_ada_boost.best_params_)
print("Best Score",clf_ada_boost.best_score_ )

DTC = DecisionTreeClassifier(random_state = 1,max_depth = 8,min_samples_leaf=5)
abc_model=AdaBoostClassifier(base_estimator = DTC,n_estimators=50, learning_rate=0.1)
#Fitting the classifier on the train data
abc_model.fit(X_train_std, y_train)

#getting the predictions on the test data
pred=abc_model.predict(X_test_std)
abc_accuracy = accuracy_score(y_test,pred)
print("Accuracy Score test :",abc_accuracy)

#Displaying the Confusion Matrix using heat maps
class_names=['0','1','2','3','4']
df_cm = pd.DataFrame(
    confusion_matrix(y_test,pred), index=class_names, columns=class_names, # creating a dataframe from the confusion matrix
    )
fig = plt.figure(figsize=(10,5)) #figure size is set
heatmap = sns.heatmap(df_cm, annot=True, fmt="d") # genrating heat map
heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=15) #Providing labels
heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=15) #Providing labels
plt.title('Confusion Matrix',fontsize=30)
plt.ylabel('Actual',fontsize=25)
plt.xlabel('Predicted',fontsize=25)
plt.show()

"""### XGBClassifier"""

#XGBClassiifier model created for class weight as balanced and using a sample of 70% records and 70% features
xgb=XGBClassifier(class_weight='balanced',subsample=0.7, colsample_bytree=0.7, random_state=42,n_jobs=-1)
#getting the list of params to be used for finding the appropriate parameters in cross validation
params={'n_estimators':[1,10,100,500,1000], 'learning_rate':[0.1,0.01,1,5,10], 'max_depth':[1,10,50,100,500,1000] }
#using grid SearchCV classifier to perform crossvalidation on the XGBoost classifier
clf_xgb=GridSearchCV(xgb, param_grid=params, n_jobs=-1, scoring='accuracy')
#fitting the model on the train data
clf_xgb.fit(X_train_std, y_train)

#Getting the best parameters and scores
print("Best Params",clf_xgb.best_params_)
print("Best Score",clf_xgb.best_score_ )

#Building the XGBoost model using the above parameters
xgb=XGBClassifier(learning_rate= 0.01, max_depth= 50, n_estimators=1000,subsample=0.7, colsample_bytree=0.7, random_state=1,n_jobs=-1)

#fitting the model on the training data
xgb.fit(X_train_std, y_train)

#getting the predicitions by applying the model on the train data
pred=xgb.predict(X_train_std)
accuracy_xgb_train = accuracy_score(y_train,pred)
print("Accuracy Score train :%s"%accuracy_xgb_train)

#getting the predicitions by applying the model on the test data
pred=xgb.predict(X_test_std)
accuracy_xgb = accuracy_score(y_test,pred)
print("Accuracy Score test :%s"%accuracy_xgb)

#Displaying the Confusion Matrix using heat maps
class_names=['0','1','2','3','4']
df_cm = pd.DataFrame(
    confusion_matrix(y_test,pred), index=class_names, columns=class_names, # creating a dataframe from the confusion matrix
    )
fig = plt.figure(figsize=(10,5)) #figure size is set
heatmap = sns.heatmap(df_cm, annot=True, fmt="d") # genrating heat map
heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=15) #Providing labels
heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=15) #Providing labels
plt.title('Confusion Matrix',fontsize=30)
plt.ylabel('Actual',fontsize=25)
plt.xlabel('Predicted',fontsize=25)
plt.show()

"""#### Catboost"""

!pip install catboost

from catboost import CatBoostClassifier
#CatboostClassiifier model created for class weight as balanced and using a sample of 70% records and 70% features
cb=CatBoostClassifier( random_state=42)
#getting the list of params to be used for finding the appropriate parameters in corss validation
params={'n_estimators':[1,10,100,500,1000], 'learning_rate':[0.1,0.01,1,5,10], 'max_depth':[1,10,50,100,500,1000] }
#using grid SearchCV classifier to perform crossvalidation on the XGBoost classifier
clf_cb=GridSearchCV(cb, param_grid=params, n_jobs=-1, scoring='accuracy')
#fitting the model on the train data
clf_cb.fit(X_train_std, y_train)

#Getting the best parameters and scores
print("Best Params",clf_cb.best_params_)
print("Best Score",clf_cb.best_score_ )

cb = clf_cb.best_estimator_
cb.fit(X_train_std,y_train)

#getting the predicitions by applying the model on the test data
pred=cb.predict(X_train_std)
accuracy_cb_train = accuracy_score(y_train,pred)
print("Accuracy Score train :%s"%accuracy_cb_train)

#getting the predicitions by applying the model on the test data
pred=cb.predict(X_test_std)
accuracy_cb = accuracy_score(y_test,pred)
print("Accuracy Score :%s"%accuracy_cb)

#Displaying the Confusion Matrix using heat maps
class_names=['0','1','2','3','4']
df_cm = pd.DataFrame(
    confusion_matrix(y_test,pred), index=class_names, columns=class_names, # creating a dataframe from the confusion matrix
    )
fig = plt.figure(figsize=(10,5)) #figure size is set
heatmap = sns.heatmap(df_cm, annot=True, fmt="d") # genrating heat map
heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=15) #Providing labels
heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=15) #Providing labels
plt.title('Confusion Matrix',fontsize=30)
plt.ylabel('Actual',fontsize=25)
plt.xlabel('Predicted',fontsize=25)
plt.show()

"""## Naive Bayes"""

param_grid_nb = {
    'var_smoothing': np.logspace(0,-9, num=100)
}

nbModel_grid = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid_nb, verbose=1, cv=100, n_jobs=-1)

nbModel_grid.fit(X_train_std, y_train)

nbModel_grid.best_params_

pred=model_nb.predict(X_train_std)
accuracy_naivebayes_train =accuracy_score(y_train,pred) 
print("Accuracy Score train :%s"%accuracy_naivebayes_train )

model_nb = nbModel_grid.best_estimator_
pred=model_nb.predict(X_test_std)
accuracy_naivebayes =accuracy_score(y_test,pred) 
print("Accuracy Score test :%s"%accuracy_naivebayes)

#Displaying the Confusion Matrix using heat maps
class_names=['0','1','2','3','4']
df_cm = pd.DataFrame(
    confusion_matrix(y_test,pred), index=class_names, columns=class_names, # creating a dataframe from the confusion matrix
    )
fig = plt.figure(figsize=(10,5)) #figure size is set
heatmap = sns.heatmap(df_cm, annot=True, fmt="d") # genrating heat map
heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=15) #Providing labels
heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=15) #Providing labels
plt.title('Confusion Matrix',fontsize=30)
plt.ylabel('Actual',fontsize=25)
plt.xlabel('Predicted',fontsize=25)
plt.show()

"""##SVM """

params_grid = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],
                     'C': [1, 10, 100, 1000]},
                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]

svm_model = GridSearchCV(SVC(), params_grid, cv=5)
svm_model.fit(X_train_std, y_train)

print('Best C from grid search above:',svm_model.best_estimator_.C) 
print('Best Kernel from grid search above:',svm_model.best_estimator_.kernel)
print('Best Gamma from grid search above:',svm_model.best_estimator_.gamma)

svm_model.best_params_

pred = final_model.predict(X_train_std)
accuracy_svm_train = accuracy_score(y_train,pred)
print("Accuracy Score of train :",accuracy_svm_train)

final_model = svm_model.best_estimator_
pred = final_model.predict(X_test_std)
accuracy_svm = accuracy_score(y_test,pred)
print("Accuracy Score of test :",accuracy_svm)

#Displaying the Confusion Matrix using heat maps
class_names=['0','1','2','3','4']
df_cm = pd.DataFrame(
    confusion_matrix(y_test,pred), index=class_names, columns=class_names, # creating a dataframe from the confusion matrix
    )
fig = plt.figure(figsize=(10,5)) #figure size is set
heatmap = sns.heatmap(df_cm, annot=True, fmt="d") # genrating heat map
heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=15) #Providing labels
heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=15) #Providing labels
plt.title('Confusion Matrix',fontsize=30)
plt.ylabel('Actual',fontsize=25)
plt.xlabel('Predicted',fontsize=25)
plt.show()

"""# lightgbm"""

import lightgbm as lgb
lg = lgb.LGBMClassifier(silent=False)
grid_param= {'learning_rate': [0.01,0.05,0.1], 'n_estimators': [8, 24,64,100], 'num_leaves': [6, 8, 12, 16,32,64,100],
                   "max_depth": [25,50, 75],'boosting_type': ['gbdt'],  'seed': [500],
                   'colsample_bytree': [0.65, 0.75, 0.8], 'subsample': [0.7, 0.75], 'reg_alpha': [1, 2, 6],
                   'reg_lambda': [1, 2, 6]}
lgb_gs= GridSearchCV(lg, n_jobs=-1, param_grid=grid_param, cv = 2, scoring="accuracy", verbose=5)
lgb_gs.fit(X_train_std, y_train)
lgb_gs.best_estimator_

#Getting the best parameters and scores
print("Best Params",lgb_gs.best_params_)
print("Best Score",lgb_gs.best_score_ )

model_lgbm = lgb_gs.best_estimator_
model_lgbm.fit(X_train_std, y_train)

pred=model_lgbm.predict(X_train_std)
accuracy_lgbm_train = accuracy_score(y_train,pred)
print("Accuracy Score of train :",accuracy_lgbm_train)

pred=model_lgbm.predict(X_test_std)
accuracy_lgbm = accuracy_score(y_test,pred)
print("Accuracy Score of test :",accuracy_lgbm)

#Displaying the Confusion Matrix using heat maps
class_names=['0','1','2','3','4']
df_cm = pd.DataFrame(
    confusion_matrix(y_test,pred), index=class_names, columns=class_names, # creating a dataframe from the confusion matrix
    )
fig = plt.figure(figsize=(10,5)) #figure size is set
heatmap = sns.heatmap(df_cm, annot=True, fmt="d") # genrating heat map
heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=15) #Providing labels
heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=15) #Providing labels
plt.title('Confusion Matrix',fontsize=30)
plt.ylabel('Actual',fontsize=25)
plt.xlabel('Predicted',fontsize=25)
plt.show()

"""#RandomForestClassifier"""

from sklearn.ensemble import RandomForestRegressor
rfc=RandomForestClassifier(random_state=1)
param_grid = { 
    'n_estimators': [200, 500],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth' : [4,5,6,7,8],
    'criterion' :['gini', 'entropy']
}
CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)
CV_rfc.fit(X_train_std, y_train)

#Getting the best parameters and scores
print("Best Params",CV_rfc.best_params_)
print("Best Score",CV_rfc.best_score_ )

rfc1=RandomForestClassifier(random_state=1, max_features='auto', n_estimators= 500, max_depth=64, criterion='gini')
rfc1.fit(X_train_std, y_train)

pred=rfc1.predict(X_train_std)
accuracy_rf_train = accuracy_score(y_train,pred)
print("Accuracy Score of train :",accuracy_rf_train)

pred=rfc1.predict(X_test_std)
accuracy_rf = accuracy_score(y_test,pred)
print("Accuracy Score of test :",accuracy_rf)

#Displaying the Confusion Matrix using heat maps
class_names=['0','1','2','3','4']
df_cm = pd.DataFrame(
    confusion_matrix(y_test,pred), index=class_names, columns=class_names, # creating a dataframe from the confusion matrix
    )
fig = plt.figure(figsize=(10,5)) #figure size is set
heatmap = sns.heatmap(df_cm, annot=True, fmt="d") # genrating heat map
heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=15) #Providing labels
heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=15) #Providing labels
plt.title('Confusion Matrix',fontsize=30)
plt.ylabel('Actual',fontsize=25)
plt.xlabel('Predicted',fontsize=25)
plt.show()

"""#SUMMARY OF ML MODEL """

from tabulate import tabulate


table = [["XGBoost",accuracy_xgb_train,accuracy_xgb,clf_xgb.best_params_],
         ["RandomForest",accuracy_rf_train, accuracy_rf, CV_rfc.best_params_],
         ["CatBoost", accuracy_cb_train,accuracy_cb, clf_cb.best_params_],
         ["NaiveBayes",accuracy_naivebayes_train,accuracy_naivebayes,nbModel_grid.best_params_],
         ["lightGbm",accuracy_lgbm_train ,accuracy_lgbm,lgb_gs.best_params_],
         ["SVM",accuracy_svm_train,accuracy_svm,svm_model.best_params_],
         ["AdaBoost",clf_ada_boost.best_score_,abc_accuracy,clf_ada_boost.best_params_]]
print(tabulate(table,headers=["ML_Algo","Training_accuracy","Test_accuracy","Best_Params"],tablefmt="grid"))

"""# 2. Deep Learning Algorithms


"""

#Loading the libraries

import tensorflow
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation, Dense, BatchNormalization,Dropout
from tensorflow.keras import optimizers
from keras.layers import LeakyReLU
#from keras import losses
from sklearn.utils import shuffle
from keras.regularizers import l2
from keras.optimizers import SGD
from keras.optimizers import RMSprop

import keras
from keras import layers
from keras import models
from keras import utils

from keras.callbacks import History, EarlyStopping
tensorflow.__version__

from sklearn import preprocessing

X_train = preprocessing.normalize(X_train_up)
X_test = preprocessing.normalize(X_test)
X_cv = preprocessing.normalize(X_cv)



# Convert class vectors to binary class matrices
num_classes=5
y_train = utils.np_utils.to_categorical(y_train_up, num_classes)
y_test = utils.np_utils.to_categorical(y_test, num_classes)
y_cv = utils.np_utils.to_categorical(y_cv, num_classes)

model = Sequential()
model.add(Dense(2048, input_shape = (80,)))
model.add(BatchNormalization())
model.add(LeakyReLU(alpha=0.1))
model.add(Dense(1024,  activation = 'tanh'))
#model.add(BatchNormalization())
#model.add(Dense(1024,  activation = 'tanh'))
#model.add(Dropout(0.1))
model.add(Dense(5, activation = 'softmax'))

#Setting the details of different parameters
epochs=1000
learning_rate = 0.001
decay_rate = learning_rate/1000 
momentum = 0.8

#EarlyStopping values
EarlyStoppingearlystop = EarlyStopping(monitor = 'val_loss',
                          min_delta = 0.0,
                          patience = 200,
                          verbose = 1,
                          restore_best_weights = True)
annealer = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2 ,patience=30, verbose=1, min_lr=1e-5)

#sgd = optimizers.SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=True)
adm=optimizers.Adam(learning_rate=learning_rate)

model.compile(loss='categorical_crossentropy', optimizer=adm , metrics=['accuracy'])

history = model.fit(X_train, y_train, batch_size = 10, epochs = 1000, verbose = 1,callbacks=[EarlyStoppingearlystop,annealer] , validation_data=(X_cv, y_cv))

model.evaluate(X_test, y_test)

"""# 3. Train, Test using Bidirectional LSTM & Glove

# Define parameters
"""

#maximum number of words to be considered 
MAX_FEATURES = 35000 
#Lets keep maxlength of Description to 100, add padding to Desription with less than 100 words and truncating long ones
MAX_DESCRIPTION_LENGTH = 100

#embedding_size
EMBEDDING_SIZE=128

"""# Prepare features and labels


"""

# Select input and output features
X_text = df['Description_processed']
y_text = df['Accident Level']

# Divide our data into testing and training sets:
X_text_train, X_text_test, y_text_train, y_text_test = train_test_split(X_text, y_text, test_size = 0.20, random_state = 1, stratify = y_text)

print('X_text_train shape : ({0})'.format(X_text_train.shape[0]))
print('y_text_train shape : ({0},)'.format(y_text_train.shape[0]))
print('X_text_test shape : ({0})'.format(X_text_test.shape[0]))
print('y_text_test shape : ({0},)'.format(y_text_test.shape[0]))

# Convert both the training and test labels into one-hot encoded vectors:
y_text_train = np_utils.to_categorical(y_text_train)
y_text_test = np_utils.to_categorical(y_text_test)

"""# Create indices


"""

# The first step in word embeddings is to convert the words into thier corresponding numeric indexes.
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X_text_train)

X_text_train = tokenizer.texts_to_sequences(X_text_train)
X_text_test = tokenizer.texts_to_sequences(X_text_test)



"""# Create weight matrix using Glove embeddings


"""

# Sentences can have different lengths, and therefore the sequences returned by the Tokenizer class also consist of variable lengths.
# We need to pad the our sequences using the max length.
vocab_size = len(tokenizer.word_index) + 1
print("vocab_size:", vocab_size)

maxlen = 100

X_text_train = pad_sequences(X_text_train, padding='post', maxlen=maxlen)
X_text_test = pad_sequences(X_text_test, padding='post', maxlen=maxlen)

EMBEDDING_FILE = '/content/drive/MyDrive/Colab Notebooks/EDA_NLP2_Capstone/glove.6B.200d.txt'

# We need to load the built-in GloVe word embeddings
embedding_size = 200
embeddings_dictionary = dict()

glove_file = open(EMBEDDING_FILE, encoding="utf8")

for line in glove_file:
    records = line.split()
    word = records[0]
    vector_dimensions = np.asarray(records[1:], dtype='float32')
    embeddings_dictionary[word] = vector_dimensions

glove_file.close()

embedding_matrix = np.zeros((vocab_size, embedding_size))

for word, index in tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

len(embeddings_dictionary.values())

import tensorflow as tf
os.environ['PYTHONHASHSEED']=str(7)

# Reproduce the results
def reset_random_seeds():
   os.environ['PYTHONHASHSEED']=str(7)
   #np.random.seed(7)
   #random.seed(7)
   tf.random.set_seed(7)

reset_random_seeds()

# Build a LSTM Neural Network
deep_inputs = Input(shape=(maxlen,))
embedding_layer = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], trainable=False)(deep_inputs)

LSTM_Layer_1 = Bidirectional(LSTM(128, return_sequences = True))(embedding_layer)
max_pool_layer_1 = GlobalMaxPool1D()(LSTM_Layer_1)
drop_out_layer_1 = Dropout(0.5, input_shape = (256,))(max_pool_layer_1)
dense_layer_1 = Dense(128, activation = 'relu')(drop_out_layer_1)
drop_out_layer_2 = Dropout(0.5, input_shape = (128,))(dense_layer_1)
dense_layer_2 = Dense(64, activation = 'relu')(drop_out_layer_2)
drop_out_layer_3 = Dropout(0.5, input_shape = (64,))(dense_layer_2)

dense_layer_3 = Dense(32, activation = 'relu')(drop_out_layer_3)
drop_out_layer_4 = Dropout(0.5, input_shape = (32,))(dense_layer_3)

dense_layer_4 = Dense(10, activation = 'relu')(drop_out_layer_4)
drop_out_layer_5 = Dropout(0.5, input_shape = (10,))(dense_layer_4)

dense_layer_5 = Dense(5, activation='softmax')(drop_out_layer_5)
#dense_layer_3 = Dense(5, activation='softmax')(drop_out_layer_3)

# LSTM_Layer_1 = LSTM(128)(embedding_layer)
# dense_layer_1 = Dense(5, activation='softmax')(LSTM_Layer_1)
# model = Model(inputs=deep_inputs, outputs=dense_layer_1)

model = Model(inputs=deep_inputs, outputs=dense_layer_5)
#model = Model(inputs=deep_inputs, outputs=dense_layer_3)

opt = SGD(lr=0.001, momentum=0.9)
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])

print(model.summary())

plot_model(model, to_file='model_plot1.png', show_shapes=True, show_dtype=True, show_layer_names=True)